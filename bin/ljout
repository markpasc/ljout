#!/usr/bin/env python

import collections
from datetime import datetime, timedelta
from functools import wraps
import hashlib
from itertools import islice
import json
import logging
import os
from os.path import join, normpath
from urllib import urlencode
from xml.etree import ElementTree
import xmlrpclib

import httplib2
from termtool import Termtool, subcommand, argument


__version__ = '1.0'


def xmlrpclib_binary_to_json(obj):
    if isinstance(obj, xmlrpclib.Binary):
        try:
            return obj.data.decode('utf8')
        except UnicodeDecodeError:
            return obj.data
    raise TypeError("Cannot encode %r as it isn't an xmlrpclib.Binary" % obj)


class LJTransport(xmlrpclib.Transport):

    user_agent = "ljout/{0}".format(__version__)


class LJServerProxy(xmlrpclib.ServerProxy):

    def __init__(self, uri, username=None, password=None):
        xmlrpclib.ServerProxy.__init__(self, uri, transport=LJTransport(), allow_none=False,
            use_datetime=True)

        self._username = username
        self._password = password

    def _ServerProxy__request(self, methodname, params):
        if (params and isinstance(params[0], collections.Mapping)
            and params[0].get('auth_method') == 'challenge' and params[0].get('auth_challenge') is None):
            args = dict(params[0])
            params = (args,) + params[1:]

            chal = None
            # Try repeatedly to get a challenge, like jbackup.pl does?
            while chal is None:
                challenge_response = self.LJ.XMLRPC.getchallenge()
                chal = challenge_response.get('challenge')

            args['ver'] = 1

            args['username'] = self._username
            args['auth_challenge'] = chal
            password = hashlib.md5()
            password.update(self._password)
            response = hashlib.md5()
            response.update(chal)
            response.update(password.hexdigest())
            args['auth_response'] = response.hexdigest()

        return xmlrpclib.ServerProxy._ServerProxy__request(self, methodname, params)


@argument('--site', default='www.livejournal.com')
@argument('--username')
@argument('--password')
class LJOut(Termtool):

    description = 'Export a LiveJournal.'

    def client(self, args):
        url = 'http://{0}/interface/xmlrpc'.format(args.site)
        client = LJServerProxy(url, args.username, args.password)
        return client

    @subcommand(help='set the username and password to export as')
    def configure(self, args):
        if not args.username:
            args.username = raw_input('Username: ')
        if not args.password:
            args.password = raw_input('Password: ')
        self.write_config_file(
            '--site', args.site,
            '--username', args.username,
            '--password', args.password,
        )
        print "Configured!"

    @subcommand(help='export journal posts')
    @argument('--journal', help="the name of the journal to export (default: user's)")
    @argument('path', help="path to the journal export")
    def events(self, args):
        if args.journal is None:
            args.journal = args.username

        events_dir = normpath(join(os.getcwd(), args.path, 'events'))
        if not os.access(events_dir, os.W_OK):
            os.makedirs(events_dir)

        lastsync_filename = join(args.path, 'events_lastsync.txt')
        try:
            with open(lastsync_filename, 'r') as f:
                lastsync = f.read().strip()
        except IOError:
            lastsync = '0'

        client = self.client(args)
        done = False
        while not done:
            try:
                lastsync, done = self.sync_events_since(client, lastsync, args)
            except xmlrpclib.Fault, fault:
                # If the fault was "Client is making repeated requests," knock a second off
                # the lastsync time so the isn't repeated. This could go on for a while
                # though so hopefully it only happens while debugging.
                if fault.faultCode != 406:
                    raise
                logging.warn("OOPS: Faking lastsync time to circumvent getevents rate limit")
                lastsync_dt = datetime.strptime(lastsync, '%Y-%m-%d %H:%M:%S')
                lastsync_dt -= timedelta(seconds=1)
                lastsync = lastsync_dt.strftime('%Y-%m-%d %H:%M:%S')

        with open(lastsync_filename, 'w') as f:
            f.write(lastsync)

        logging.debug("Exported events!")

    def sync_events_since(self, client, lastsync, args):
        logging.debug("Syncing items since %r", lastsync)
        sync = client.LJ.XMLRPC.syncitems({
            'lastsync': lastsync,
            'usejournal': args.journal,
            'auth_method': 'challenge',
        })

        syncitems = dict()
        syncitem_lastsync = lastsync
        for item in sync['syncitems']:
            item_code, item_id = item['item'].split('-', 1)
            item_id = int(item_id)
            if item_code == 'L':
                syncitems[item_id] = item
            if item['time'] > syncitem_lastsync:
                syncitem_lastsync = item['time']

        # If there were no "L" items to sync, there are no events to get in that window of
        # syncitems, so use its last date for the next sync (now or later).
        if not syncitems:
            done = sync['count'] == sync['total']
            return syncitem_lastsync, done

        logging.debug("Getting events since %r", lastsync)
        events = client.LJ.XMLRPC.getevents({
            'auth_method': 'challenge',
            'usejournal': args.journal,
            'selecttype': 'syncitems',
            'lastsync': lastsync,
            'lineendings': 'unix',
        })

        for event in events['events']:
            item_id = event['itemid']
            syncitem = syncitems.pop(item_id)
            if syncitem['time'] > lastsync:
                lastsync = syncitem['time']

            self.export_event(event, args)

        # If it's the last syncitems window AND we used up all the L syncitems, we finished.
        done = sync['count'] == sync['total'] and not syncitems
        return lastsync, done

    def export_event(self, event, args):
        item_id = event['itemid']
        try:
            json_str = json.dumps(event, sort_keys=True, indent=4, default=xmlrpclib_binary_to_json)
        except TypeError:
            from pprint import pformat
            raise TypeError("Could not JSON serialize event: %s" % pformat(event))
        with open(join(args.path, 'events', '{0}.json'.format(item_id)), 'w') as f:
            f.write(json_str)

    @subcommand(help='export all the comments from the journal')
    @argument('--journal', help="the name of the journal to export (default: user's)")
    @argument('path', help="path to the journal export")
    def comments(self, args):
        if args.journal is None:
            args.journal = args.username

        comments_dir = normpath(join(os.getcwd(), args.path, 'comments'))
        if not os.access(comments_dir, os.W_OK):
            os.makedirs(comments_dir)

        sessiondata = self.client(args).LJ.XMLRPC.sessiongenerate({
            'expiration': 'short',
            'auth_method': 'challenge',
        })
        session = sessiondata['ljsession']
        logging.debug("Got session to fetch comment data with")

        metadata, usermap = self.comment_metadata(session, args)
        self.export_comments(session, max(metadata.keys()), usermap, args)

    def comment_metadata(self, session, args):
        # Get all the fresh metadata.
        max_id = 0
        metadata, usermap = {}, {}
        while True:
            request = {
                'get': 'comment_meta',
                'startid': max_id,
                'authas': args.journal,
            }
            url = 'http://{0}/export_comments.bml?{1}'.format(args.site, urlencode(request))
            h = httplib2.Http()
            resp, cont = h.request(url, headers={
                'User-Agent': LJTransport.user_agent,
                'Cookie': 'ljsession={0}'.format(session),
            })
            assert resp.status == 200
            logging.debug("Fetched comment data starting at %d", max_id)

            doc = ElementTree.fromstring(cont)

            for comment_el in doc.findall('comments/comment'):
                comment_data = {}
                comment_id = int(comment_el.attrib['id'])
                max_id = max(max_id, comment_id)
                metadata[comment_id] = comment_data

                poster_id = comment_el.attrib.get('posterid')
                if poster_id:
                    comment_data['poster_id'] = int(poster_id)

                state = comment_el.attrib.get('state')
                if state:
                    comment_data['state'] = state

            for usermap_el in doc.findall('usermaps/usermap'):
                user_id = int(usermap_el.attrib['id'])
                user_name = usermap_el.attrib['user']
                usermap[user_id] = user_name

            # Stop once we found the last comment (the one with max_id for an id).
            last_max_id = int(doc.findtext('maxid'))
            if last_max_id in metadata:
                break

        metadata_str = json.dumps(metadata, sort_keys=True, indent=4)
        with open(join(args.path, 'comment_metadata.json'), 'w') as f:
            f.write(metadata_str)

        return metadata, usermap

    def export_comments(self, session, max_id, usermap, args):
        start_id = 0
        seen_ids = set()
        while max_id not in seen_ids:
            request = {
                'get': 'comment_body',
                'startid': start_id,
                'authas': args.journal,
                'props': '1',
            }
            url = 'http://{0}/export_comments.bml?{1}'.format(args.site, urlencode(request))
            h = httplib2.Http()
            resp, cont = h.request(url, headers={
                'User-Agent': LJTransport.user_agent,
                'Cookie': 'ljsession={0}'.format(session),
            })
            assert resp.status == 200
            logging.debug("Fetched comment bodies starting at %d", start_id)

            doc = ElementTree.fromstring(cont)

            for comment_el in doc.findall('comments/comment'):
                comment_data = dict()

                get_attr = lambda c, f: c.attrib.get(f)
                get_child = lambda c, f: c.findtext(f)
                noop = lambda x: x
                fields = {
                    'id': (get_attr, int, 'id'),
                    'jitemid': (get_attr, int, 'event_id'),
                    'posterid': (get_attr, lambda x: usermap.get(int(x)), 'poster'),
                    'state': (get_attr, noop, 'state'),
                    'body': (get_child, noop, 'comment'),
                    'date': (get_child, noop, 'date'),
                    'subject': (get_child, noop, 'subject'),
                }
                for field, buh in fields.iteritems():
                    getterate, coerce, data_field = buh
                    value = getterate(comment_el, field)
                    if value is not None:
                        value = coerce(value)
                        comment_data[data_field] = value

                props = dict()
                for prop_el in comment_el.findall('property'):
                    prop_name = prop_el.attrib['name']
                    props[prop_name] = prop_el.text
                if props:
                    comment_data['props'] = props

                comment_str = json.dumps(comment_data, sort_keys=True, indent=4)
                with open(join(args.path, 'comments', '{0}.json'.format(comment_data['id'])), 'w') as f:
                    f.write(comment_str)

                seen_ids.add(comment_data['id'])

            start_id = max(seen_ids)


if __name__ == '__main__':
    LJOut().run()
